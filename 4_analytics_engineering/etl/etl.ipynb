{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page'\n",
    "\n",
    "taxi_data_dtypes = {\n",
    "    'VendorID': pd.Int64Dtype(),\n",
    "    'lpep_pickup_datetime': 'datetime64[ns]',\n",
    "    'lpep_dropoff_datetime': 'datetime64[ns]',\n",
    "    'store_and_fwd_flag': str,\n",
    "    'RatecodeID': pd.Int64Dtype(),\n",
    "    'PULocationID': pd.Int64Dtype(),\n",
    "    'DOLocationID': pd.Int64Dtype(),\n",
    "    'passenger_count': pd.Int64Dtype(),\n",
    "    'trip_distance': float,\n",
    "    'fare_amount': float,\n",
    "    'extra': float,\n",
    "    'mta_tax': float,\n",
    "    'tip_amount': float,\n",
    "    'tolls_amount': float,\n",
    "    'improvement_surcharge': float,\n",
    "    'total_amount': float,\n",
    "    'payment_type': pd.Int64Dtype(),\n",
    "    'congestion_surcharge': float\n",
    "}\n",
    "\n",
    "\n",
    "def stream_download_parquet(url):\n",
    "    response = requests.get(url).text\n",
    "    soup = BeautifulSoup(response, features='html.parser')  # Raise an HTTPError for bad responses\n",
    "\n",
    "    for i, link in enumerate(soup.findAll('a')):\n",
    "        href = link.get('href')\n",
    "\n",
    "        if 'fhv_tripdata_2019' in href:\n",
    "            parquet_file = pd.read_parquet(href)\n",
    "            \n",
    "            yield parquet_file.astype(taxi_data_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "PipelineStepFailed",
     "evalue": "Pipeline execution failed at stage extract when processing package 1709008993.709861 with exception:\n\n<class 'dlt.extract.exceptions.ResourceExtractionError'>\nIn processing pipe stream_download: extraction of resource stream_download in generator stream_download_parquet caused an exception: \"Only a column name can be used for the key in a dtype mappings argument. 'VendorID' not found in columns.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/extract/pipe.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mResourceExtractionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"generator\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/bb/x7w4fwsj6qj4hwzsp48hpn1c0000gn/T/ipykernel_91565/1983757729.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mparquet_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mparquet_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtaxi_data_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6597\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtype_ser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6598\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6599\u001b[0;31m                     raise KeyError(\n\u001b[0m\u001b[1;32m   6600\u001b[0m                         \u001b[0;34m\"Only a column name can be used for the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Only a column name can be used for the key in a dtype mappings argument. 'VendorID' not found in columns.\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceExtractionError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:391\u001b[0m, in \u001b[0;36mPipeline.extract\u001b[0;34m(self, data, table_name, parent_table_name, write_disposition, columns, primary_key, schema, max_parallel_items, workers, schema_contract)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SourceExhausted(source\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# extract state\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:1028\u001b[0m, in \u001b[0;36mPipeline._extract_source\u001b[0;34m(self, extract, source, max_parallel_items, workers)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;66;03m# extract into pipeline schema\u001b[39;00m\n\u001b[0;32m-> 1028\u001b[0m load_id \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# save import with fully discovered schema\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/extract/extract.py:341\u001b[0m, in \u001b[0;36mExtract.extract\u001b[0;34m(self, source, max_parallel_items, workers)\u001b[0m\n\u001b[1;32m    339\u001b[0m                     reset_resource_state(resource\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 341\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_single_source\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_parallel_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_parallel_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_id\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/extract/extract.py:271\u001b[0m, in \u001b[0;36mExtract._extract_single_source\u001b[0;34m(self, load_id, source, max_parallel_items, workers, futures_poll_interval)\u001b[0m\n\u001b[1;32m    270\u001b[0m collector\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResources\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, total_gens)\n\u001b[0;32m--> 271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pipe_item \u001b[38;5;129;01min\u001b[39;00m pipes:\n\u001b[1;32m    272\u001b[0m     curr_gens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pipes\u001b[38;5;241m.\u001b[39m_sources)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/extract/pipe.py:605\u001b[0m, in \u001b[0;36mPipeIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipe_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     pipe_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_source_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipe_item \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/extract/pipe.py:863\u001b[0m, in \u001b[0;36mPipeIterator._get_source_item\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResourceExtractionError(pipe\u001b[38;5;241m.\u001b[39mname, gen, \u001b[38;5;28mstr\u001b[39m(ex), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mex\u001b[39;00m\n",
      "\u001b[0;31mResourceExtractionError\u001b[0m: In processing pipe stream_download: extraction of resource stream_download in generator stream_download_parquet caused an exception: \"Only a column name can be used for the key in a dtype mappings argument. 'VendorID' not found in columns.\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m generators_pipeline \u001b[38;5;241m=\u001b[39m dlt\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m      6\u001b[0m     destination\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduckdb\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerators\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# we can load the next generator to the same or to a different table.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[43mgenerators_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_download_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_download\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreplace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(info)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:193\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    191\u001b[0m         trace_step \u001b[38;5;241m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:238\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[1;32m    236\u001b[0m         ConfigSectionContext(pipeline_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, sections\u001b[38;5;241m=\u001b[39msections)\n\u001b[1;32m    237\u001b[0m     ):\n\u001b[0;32m--> 238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:619\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, data, destination, staging, dataset_name, credentials, table_name, write_disposition, columns, primary_key, schema, loader_file_format, schema_contract)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# extract from the source\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrite_disposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_disposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprimary_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mschema_contract\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema_contract\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize(loader_file_format\u001b[38;5;241m=\u001b[39mloader_file_format)\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(destination, dataset_name, credentials\u001b[38;5;241m=\u001b[39mcredentials)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:193\u001b[0m, in \u001b[0;36mwith_runtime_trace.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trace:\n\u001b[1;32m    191\u001b[0m         trace_step \u001b[38;5;241m=\u001b[39m start_trace_step(trace, cast(TPipelineStep, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 193\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m step_info\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:164\u001b[0m, in \u001b[0;36mwith_schemas_sync.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mlive_schemas:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# refresh live schemas in storage or import schema path\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mcommit_live_schema(name)\n\u001b[0;32m--> 164\u001b[0m rv \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# save modified live schemas\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema_storage\u001b[38;5;241m.\u001b[39mlive_schemas:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:151\u001b[0m, in \u001b[0;36mwith_state_sync.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_state(extract_state\u001b[38;5;241m=\u001b[39mshould_extract_state) \u001b[38;5;28;01mas\u001b[39;00m state:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# add the state to container as a context\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container\u001b[38;5;241m.\u001b[39minjectable_context(StateInjectableContext(state\u001b[38;5;241m=\u001b[39mstate)):\n\u001b[0;32m--> 151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:238\u001b[0m, in \u001b[0;36mwith_config_section.<locals>.decorator.<locals>._wrap\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# add section context to the container to be used by all configuration without explicit sections resolution\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inject_section(\n\u001b[1;32m    236\u001b[0m         ConfigSectionContext(pipeline_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, sections\u001b[38;5;241m=\u001b[39msections)\n\u001b[1;32m    237\u001b[0m     ):\n\u001b[0;32m--> 238\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/dlt/lib/python3.10/site-packages/dlt/pipeline/pipeline.py:403\u001b[0m, in \u001b[0;36mPipeline.extract\u001b[0;34m(self, data, table_name, parent_table_name, write_disposition, columns, primary_key, schema, max_parallel_items, workers, schema_contract)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    402\u001b[0m     step_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_step_info(extract_step)\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PipelineStepFailed(\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextract\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    406\u001b[0m         extract_step\u001b[38;5;241m.\u001b[39mcurrent_load_id,\n\u001b[1;32m    407\u001b[0m         exc,\n\u001b[1;32m    408\u001b[0m         step_info,\n\u001b[1;32m    409\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mPipelineStepFailed\u001b[0m: Pipeline execution failed at stage extract when processing package 1709008993.709861 with exception:\n\n<class 'dlt.extract.exceptions.ResourceExtractionError'>\nIn processing pipe stream_download: extraction of resource stream_download in generator stream_download_parquet caused an exception: \"Only a column name can be used for the key in a dtype mappings argument. 'VendorID' not found in columns.\""
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "\n",
    "# define the connection to load to.\n",
    "# We now use duckdb, but you can switch to Bigquery later\n",
    "generators_pipeline = dlt.pipeline(\n",
    "    destination='duckdb',\n",
    "    dataset_name='generators'\n",
    ")\n",
    "\n",
    "# we can load the next generator to the same or to a different table.\n",
    "info = generators_pipeline.run(\n",
    "    stream_download_parquet(url),\n",
    "    table_name=\"stream_download\",\n",
    "    write_disposition=\"replace\"\n",
    ")\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "conn = duckdb.connect(f\"{generators_pipeline.pipeline_name}.duckdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tables:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌─────────────────────┐\n",
       "│        name         │\n",
       "│       varchar       │\n",
       "├─────────────────────┤\n",
       "│ _dlt_loads          │\n",
       "│ _dlt_pipeline_state │\n",
       "│ _dlt_version        │\n",
       "│ stream_download     │\n",
       "└─────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show tables\n",
    "conn.sql(f\"SET search_path = '{generators_pipeline.dataset_name}'\")\n",
    "print('Loaded tables:')\n",
    "display(conn.sql(\"show tables\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_download table below:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌───────────┬──────────────────────┬──────────────────────┬───┬──────────────┬───────────┬──────────────────────┐\n",
       "│ vendor_id │ lpep_pickup_datetime │ lpep_dropoff_datet…  │ … │ payment_type │ trip_type │ congestion_surcharge │\n",
       "│   int64   │ timestamp with tim…  │ timestamp with tim…  │   │    int64     │  double   │        double        │\n",
       "├───────────┼──────────────────────┼──────────────────────┼───┼──────────────┼───────────┼──────────────────────┤\n",
       "│         2 │ 2018-12-21 08:17:2…  │ 2018-12-21 08:18:5…  │ … │            2 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:10:1…  │ 2018-12-31 17:16:3…  │ … │            2 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:27:1…  │ 2018-12-31 17:31:3…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:46:2…  │ 2018-12-31 18:04:5…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:19:0…  │ 2018-12-31 17:39:4…  │ … │            2 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:12:3…  │ 2018-12-31 17:19:0…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:47:5…  │ 2018-12-31 18:00:0…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         1 │ 2018-12-31 17:12:4…  │ 2018-12-31 17:30:5…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:16:2…  │ 2018-12-31 17:39:4…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2018-12-31 17:58:0…  │ 2018-12-31 18:19:0…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         · │          ·           │          ·           │ · │            · │        ·  │                   ·  │\n",
       "│         · │          ·           │          ·           │ · │            · │        ·  │                   ·  │\n",
       "│         · │          ·           │          ·           │ · │            · │        ·  │                   ·  │\n",
       "│         2 │ 2019-01-01 08:42:4…  │ 2019-01-01 08:57:3…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         1 │ 2019-01-01 08:03:0…  │ 2019-01-01 08:21:5…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         1 │ 2019-01-01 08:36:0…  │ 2019-01-01 09:09:1…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2019-01-01 08:09:5…  │ 2019-01-01 08:38:4…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2019-01-01 08:30:1…  │ 2019-01-01 08:36:3…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2019-01-01 08:46:0…  │ 2019-01-01 08:56:3…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2019-01-01 08:27:1…  │ 2019-01-01 08:40:4…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         2 │ 2019-01-01 09:01:0…  │ 2019-01-01 09:14:2…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         1 │ 2019-01-01 08:01:4…  │ 2019-01-01 08:09:5…  │ … │            1 │       1.0 │                 NULL │\n",
       "│         1 │ 2019-01-01 08:29:4…  │ 2019-01-01 08:40:3…  │ … │            1 │       1.0 │                 NULL │\n",
       "├───────────┴──────────────────────┴──────────────────────┴───┴──────────────┴───────────┴──────────────────────┤\n",
       "│ ? rows (>9999 rows, 20 shown)                                                            20 columns (6 shown) │\n",
       "└───────────────────────────────────────────────────────────────────────────────────────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data\n",
    "print(\"stream_download table below:\")\n",
    "display(conn.sql(\"SELECT * FROM stream_download\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_download count of records:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌──────────────┐\n",
       "│ count_star() │\n",
       "│    int64     │\n",
       "├──────────────┤\n",
       "│      6300985 │\n",
       "└──────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# count records\n",
    "print(\"stream_download count of records:\")\n",
    "display(conn.sql(\"SELECT COUNT(*) FROM stream_download\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lpep_pickup_date</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-10-21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>2020-03-29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>2020-04-22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>2035-09-02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>2062-08-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>388 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    lpep_pickup_date  count\n",
       "0         2008-10-21      1\n",
       "1         2008-12-31     98\n",
       "2         2009-01-01     11\n",
       "3         2009-01-04      1\n",
       "4         2009-01-05      1\n",
       "..               ...    ...\n",
       "383       2020-03-29      1\n",
       "384       2020-04-01      1\n",
       "385       2020-04-22      2\n",
       "386       2035-09-02      1\n",
       "387       2062-08-14      1\n",
       "\n",
       "[388 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_group_by_date = '''\n",
    "    SELECT\n",
    "        lpep_pickup_datetime::DATE AS lpep_pickup_date,\n",
    "        COUNT(*) AS count\n",
    "        \n",
    "    FROM\n",
    "        stream_download\n",
    "        \n",
    "    GROUP BY\n",
    "        lpep_pickup_datetime::DATE\n",
    "        \n",
    "    ORDER BY\n",
    "        lpep_pickup_datetime::DATE\n",
    "'''\n",
    "\n",
    "duck_df = conn.sql(query_group_by_date).df()\n",
    "duck_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in duck_df['lpep_pickup_date']:\n",
    "#     df = conn.sql(f\"SELECT '{d.date()}' AS date, COUNT(*) AS count FROM stream_download WHERE lpep_pickup_datetime::DATE = '{d.date()}'\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# import pyarrow as pa\n",
    "from io import BytesIO\n",
    "\n",
    "# Load to S3\n",
    "def df_to_s3(df: pd.DataFrame, bucket: str, object_key: str) -> None:\n",
    "    session = boto3.session.Session(profile_name='dez')\n",
    "    s3_client = session.client('s3')\n",
    "\n",
    "    # s3_url = f\"s3://{bucket}/{prefix}_{d.strftime('%Y%m%d')}.parquet\"\n",
    "    # df.to_parquet(s3_url)\n",
    "\n",
    "    # https://stackoverflow.com/questions/53416226/how-to-write-parquet-file-from-pandas-dataframe-in-s3-in-python\n",
    "    out_buffer = BytesIO()\n",
    "    df.to_parquet(out_buffer, compression='gzip', index=False)\n",
    "\n",
    "    s3_client.put_object(Bucket=bucket, Key=object_key, Body=out_buffer.getvalue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bb/x7w4fwsj6qj4hwzsp48hpn1c0000gn/T/ipykernel_91565/3728565809.py:6: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df=conn.sql(f\"SELECT * FROM stream_download WHERE lpep_pickup_datetime::DATE = '{d.date()}'\").df(),\n"
     ]
    }
   ],
   "source": [
    "BUCKET = 'dez2024-dez-dlt'\n",
    "PREFIX = 'ny_taxi/green/2019'\n",
    "\n",
    "for d in duck_df['lpep_pickup_date']:\n",
    "    df_to_s3(\n",
    "        df=conn.sql(f\"SELECT * FROM stream_download WHERE lpep_pickup_datetime::DATE = '{d.date()}'\").df(),\n",
    "        bucket=BUCKET,\n",
    "        object_key=f\"{PREFIX}/{d.strftime('%Y%m%d')}.parquet.gz\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
