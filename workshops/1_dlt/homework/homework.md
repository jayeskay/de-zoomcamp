# Workshop 1 Homework

​In this hands-on workshop, we’ll learn how to build data ingestion pipelines.

​We’ll cover the following steps:

- ​Extracting data from APIs, or files.
- ​Normalizing and loading data
- ​Incremental loading

​By the end of this workshop, you’ll be able to write data pipelines like a senior data engineer: Quickly, concisely, scalable, and self-maintaining.

## Navigation

* [Workshop content](dlt_resources/data_ingestion_workshop.md)
* [Workshop notebook](dlt_resources/workshop.ipynb)
* [Homework starter notebook](dlt_resources/homework_starter.ipynb)

## Resources

- Website and community: Visit our [docs](https://dlthub.com/docs/intro), discuss on our slack (Link at top of docs).
- Course colab: [Notebook](https://colab.research.google.com/drive/1kLyD3AL-tYf_HqCXYnA3ZLwHGpzbLmoj#scrollTo=5aPjk0O3S_Ag&forceEdit=true&sandboxMode=true).
- dlthub [community Slack](https://dlthub.com/community).

## Questions

The [linked colab notebook](https://colab.research.google.com/drive/1Te-AT0lfh0GpChg1Rbd0ByEKOHYtWXfm#scrollTo=wLF4iXf-NR7t&forceEdit=true&sandboxMode=true) offers a few exercises to practice what you learned today.

### Question 1.

What is the sum of the outputs of the generator for limit = 5?

- 10.23433234744176
- 7.892332347441762
- **8.382332347441762** &larr;
- 9.123332347441762

#### Explanation

Using the provided function,

```python
def square_root_generator(limit):
    n = 1
    while n <= limit:
        yield n ** 0.5
        n += 1
```

I set `limit = 5` and stored the generator object in variable, `generator`. From here, I initialized variable `s = 0` and used a `for` loop to get values from generator and increment `s` with the output:

```python
for sqrt_value in generator:
    s += sqrt_value
    print(f"sqrt_value={sqrt_value}; sum={s}")
```

Output:

```
sqrt_value=1.0; sum=1.0
sqrt_value=1.4142135623730951; sum=2.414213562373095
sqrt_value=1.7320508075688772; sum=4.146264369941973
sqrt_value=2.0; sum=6.146264369941973
sqrt_value=2.23606797749979; sum=8.382332347441762
```

### Question 2.

What is the 13th number yielded by the generator?

- 4.236551275463989
- **3.605551275463989** &larr;
- 2.345551275463989
- 5.678551275463989

#### Explanation

I modified the `square_root_generator()` function to change scope of `n` to global:

```python
def square_root_generator_global(limit):
    global n
    n = 1
    while n <= limit:
        yield n ** 0.5
        n += 1
```

This allowed me to call `n` in the `print()` statement directly after setting `limit = 13` this time:

```python
for sqrt_value in generator:
    print(f"{n}:\t {sqrt_value}")
```

Thus, output:

```
1:	 1.0
2:	 1.4142135623730951
3:	 1.7320508075688772
4:	 2.0
5:	 2.23606797749979
6:	 2.449489742783178
7:	 2.6457513110645907
8:	 2.8284271247461903
9:	 3.0
10:	 3.1622776601683795
11:	 3.3166247903554
12:	 3.4641016151377544
13:	 3.605551275463989
```

### Question 3.

Append the 2 generators. After correctly appending the data, calculate the sum of all ages of people.

- **353** &larr;
- 365
- 378
- 390

#### Explanation

Here's the first generator:

```python
def people_1():
    for i in range(1, 6):
        yield {"ID": i, "Name": f"Person_{i}", "Age": 25 + i, "City": "City_A"}
```

And the second:

```python
def people_2():
    for i in range(3, 9):
        yield {"ID": i, "Name": f"Person_{i}", "Age": 30 + i, "City": "City_B", "Occupation": f"Job_{i}"}
```

After defining `generators_pipeline = dlt.pipeline(destination='duckdb', dataset_name='generators')`, the below can be executed to load generator data to local duckDB instance:

```python
generators_pipeline.run(
    people_1(),
    table_name="people_combined",
    write_disposition="replace"
)

generators_pipeline.run(
    people_2(),
    table_name="people_combined",
    write_disposition="append"
)
```

Notice that `table_name` parameter is same for both runs, but whereas the disposition is `replace` for `people_1()`, it's `append` for `people_2()`. This allows data generated by `people_2()` to be appended to that generated afresh by `people_1()` each time. It will not continue writing to an existing table each time the script/cell is executed.

To see output as a Pandas DataFrame object:

```python
display(conn.sql("SELECT * FROM people_comobined").df())
```

It's possible, however, to get the sum right from the table in duckDB. The below yields **353.0**.

```python
display(conn.sql("SELECT SUM(age) FROM people_combined"))
```

### Question 4.

Merge the 2 generators using the ID column. Calculate the sum of ages of all the people loaded as described above.

- 215
- **266** &larr;
- 241
- 258

#### Explanation

I used the same generators as above, but I had to use an f-string in the connection object:

```python
generators_pipeline = dlt.pipeline(destination='duckdb', dataset_name='generators_primary_keys')
conn = duckdb.connect(f"{generators_pipeline.pipeline_name}.duckdb")
```

I ran as follows:

```python
generators_pipeline.run(
    people_1(),
    table_name="people_pk",
    write_disposition="replace",
    primary_key="id"
)
```

The above specifies a primary key, "id", from the generator.

Displaying the data, however, also required an f-string:

```python
display(conn.sql(f"SELECT * FROM {generators_pipeline.dataset_name}.people_pk"))
```

Instead of appending data from `people_2()` generator, this time I merged on the primary key specified in run for `people_1()`:

```python
generators_pipeline.run(
    people_2(),
    table_name="people_pk",
    write_disposition="merge",
    primary_key="id"
)
```

Again, because the `write_disposition` for `people_1()` run is "replace", I can execute this process as many times as I like without applying changes to the same table repeatedly--it will be recreated each time.

Lastly, the sum was calculated as follows, resulting in **266.0**:

```python
display(conn.sql(f"SELECT SUM(age) FROM {generators_pipeline.dataset_name}.people_pk"))
```
